{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BANK MARKETING\n",
    "\n",
    "<br><br>\n",
    "Membros:\n",
    "- Anderson Jesus\n",
    "- Caio Viera\n",
    "- Pedro Correia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CRIAÇÃO DE MODELO PREDITIVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando sessão do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/labdata/spark-2.2.1-bin-hadoop2.6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('bank').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    \"hdfs://elephant:8020/user/labdata/bank-historical-data.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.selectExpr(*[\"`{}` as {}\".format(col, col.replace('.', '_')) for col in data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=35, job='admin.', marital='single', education='university.degree', default='no', housing='yes', loan='no', contact='cellular', month='may', day_of_week='tue', duration=255, campaign=2, pdays=999, previous=1, poutcome='failure', emp_var_rate=-1.8, cons_price_idx=92.89299999999999, cons_conf_idx=-46.2, euribor3m=1.291, nr_employed=5099.1, y='no', client_id='TRZK1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo variáveis utilizadas pelo tipo a fim de realizar o encoding necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'month',\n",
    "    'day_of_week',\n",
    "    'poutcome'\n",
    "]\n",
    "\n",
    "# não utilizaremos a variável `duration`, que algo não sabido antes da ligação ocorrer\n",
    "# e portanto, não deve ser válida para fins preditivos\n",
    "numericColumns = [\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'emp_var_rate',\n",
    "    'cons_price_idx',\n",
    "    'cons_conf_idx',\n",
    "    'euribor3m',\n",
    "    'nr_employed'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando a construção do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders e indexadores necessários ao tratamento dos dados\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# instanciando nossa lista de passos a serem fornecidos ao pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexação da variável resposta\n",
    "indexer = StringIndexer(\n",
    "    inputCol='y', \n",
    "    outputCol='label'\n",
    ").setHandleInvalid(\"skip\")\n",
    "\n",
    "stages += [indexer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados Categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformações dados categóricos (paralelo pandas: get_dummies)\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # nomes para valores [0:n_cats-1]\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=categoricalCol, \n",
    "        outputCol=categoricalCol+'_index'\n",
    "    ).setHandleInvalid(\"skip\")\n",
    "    # criando dummies\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=categoricalCol+'_index',\n",
    "        outputCol=categoricalCol+'_class_vec'\n",
    "    )\n",
    "    # inserindo estágios de transformação\n",
    "    stages += [indexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando variáveis numéricas para o tipo double\n",
    "for numericCol in numericColumns:\n",
    "    data = data.withColumn(numericCol, data[numericCol].cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando assembler, que deixa os dados no formato vetorial \n",
    "# demandado pela biblioteca ML do Spark\n",
    "\n",
    "assembler_inputs = [categoricalCol+'_class_vec' for categoricalCol in categoricalColumns]\n",
    "assembler_inputs += numericColumns\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Boosting Machine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol='prediction',\n",
    "    maxIter=60,\n",
    "    stepSize=0.01,\n",
    "    seed=420\n",
    ")\n",
    "\n",
    "stages += [gbt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline.write().overwrite().save('hdfs://elephant:8020/user/labdata/model2/bank-pipeline-model-unfit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e Validando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações para treino:   26389\n",
      "Observações para teste:     6561\n"
     ]
    }
   ],
   "source": [
    "# separação de dados em treino e teste\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=420)\n",
    "\n",
    "print('Observações para treino: {:>7}'.format(trainingData.count()))\n",
    "print('Observações para teste:  {:>7}'.format(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76.2 ms, sys: 0 ns, total: 76.2 ms\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipelineModel = pipeline.fit(trainingData)\n",
    "pipelineModel.write().overwrite().save('hdfs://elephant:8020/user/labdata/model2/bank-pipeline-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt = pipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:         0.9032\n",
      "areaUnderROC:     0.7881\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "accuracy_gbt = evaluator_accuracy.evaluate(predictions_gbt)\n",
    "print('accuracy:         {:.4f}'.format(accuracy_gbt))\n",
    "auc_gbt = evaluator_auc.evaluate(predictions_gbt)\n",
    "print(f'areaUnderROC:     {auc_gbt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.9032|   0.6656|0.2805|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt.select('label', 'prediction').createOrReplaceTempView('predictions')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GBTClassifier` apresentou maior potencial, apesar de ter um recall ainda muito baixo. \n",
    "\n",
    "Na sequência, vamos buscar realizar um resampling da base, buscando deixar a classe positiva mais prevalente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  y|count|\n",
      "+---+-----+\n",
      "| no| 5886|\n",
      "|yes| 3713|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_res = data.sampleBy('y', fractions={'yes': 1, 'no': 0.2}, seed=420)\n",
    "data_res.groupBy('y').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações para treino:    7686\n",
      "Observações para teste:     1913\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = data_res.randomSplit([0.8, 0.2], seed=420)\n",
    "\n",
    "print('Observações para treino: {:>7}'.format(trainingData.count()))\n",
    "print('Observações para teste:  {:>7}'.format(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 ms, sys: 0 ns, total: 64 ms\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# criando novo modelo com resampling\n",
    "pipelineModel_res = pipeline.fit(trainingData)\n",
    "\n",
    "# salvando o modelo\n",
    "pipelineModel_res.write().overwrite().save('hdfs://elephant:8020/user/labdata/model2/bank-pipeline-model-res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_res = pipelineModel_res.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+----------+\n",
      "|client_id|label|         probability|prediction|\n",
      "+---------+-----+--------------------+----------+\n",
      "|    99E3E|  1.0|[0.15967127685075...|       1.0|\n",
      "|    55UK3|  1.0|[0.11209598280032...|       1.0|\n",
      "|    M7AJH|  1.0|[0.10728508532343...|       1.0|\n",
      "|    3M8VA|  1.0|[0.63161893547033...|       0.0|\n",
      "|    4621G|  0.0|[0.75710971442414...|       0.0|\n",
      "|    QVAU3|  1.0|[0.90527359235893...|       0.0|\n",
      "|    350UZ|  0.0|[0.71177995924388...|       0.0|\n",
      "|    3SIFF|  1.0|[0.26985526483253...|       1.0|\n",
      "|    28JND|  1.0|[0.41451071872129...|       1.0|\n",
      "|    MQL5O|  0.0|[0.63226260473842...|       0.0|\n",
      "+---------+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_res.select('client_id', 'label', 'probability', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.7669\n",
      "areaUnderROC:     0.7881\n"
     ]
    }
   ],
   "source": [
    "accuracy_res = evaluator_accuracy.evaluate(predictions_res)\n",
    "print('Accuracy:         {:.4f}'.format(accuracy_res))\n",
    "auc_gbt = evaluator_auc.evaluate(predictions_gbt)\n",
    "print(f'areaUnderROC:     {auc_gbt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.7669|   0.7786|0.5806|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_res.select('label', 'prediction').createOrReplaceTempView('predictions_res')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions_res\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperávamos, o recall apresentou um valor bem mais interessante, demonstrando o potencial da solução, que certamente poderia ser refinada ainda mais a fim de atingir resultados mais expressivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
